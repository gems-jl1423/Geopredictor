{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Go864599g2-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8151b6a5-a3de-4e84-c29b-810513e17fd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SbmWw6YNhZYV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d64530-8aeb-4422-925b-4a16e41d9619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting livelossplot\n",
            "  Downloading livelossplot-0.5.5-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from livelossplot) (3.7.1)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.10/dist-packages (from livelossplot) (3.3.4)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (3.1.3)\n",
            "Requirement already satisfied: contourpy>=1 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (1.23.5)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (23.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (1.5.3)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (6.0.1)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (6.3.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (2023.10.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.1.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->bokeh->livelossplot) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->livelossplot) (1.16.0)\n",
            "Installing collected packages: livelossplot\n",
            "Successfully installed livelossplot-0.5.5\n",
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ],
      "source": [
        "!pip install livelossplot\n",
        "%pylab inline\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from livelossplot import PlotLosses\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader,random_split\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset, DataLoader,random_split\n",
        "from PIL import Image\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "import random\n",
        "import torch\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nc_9I6h6iGj7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d61b49f-c24f-4c50-902b-188dcf5aa1b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda installed! Running on GPU!\n"
          ]
        }
      ],
      "source": [
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = False\n",
        "\n",
        "    return True\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GkBde62AOiSI"
      },
      "outputs": [],
      "source": [
        "class NumpyImageDataset(Dataset):\n",
        "    def __init__(self, folder_path, transform=None):\n",
        "        self.folder_path = folder_path\n",
        "        self.transform = transform\n",
        "\n",
        "        # List subdirectories (categories)\n",
        "        self.categories = sorted(os.listdir(folder_path))\n",
        "\n",
        "        # List all .npy files in each subdirectory\n",
        "        self.file_paths = []\n",
        "        for category in self.categories:\n",
        "            category_path = os.path.join(folder_path, category)\n",
        "            file_names = os.listdir(category_path)\n",
        "            file_paths = [os.path.join(category_path, file_name) for file_name in file_names if file_name.endswith('.npy')]\n",
        "            self.file_paths.extend(file_paths)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load .npy file\n",
        "        file_path = self.file_paths[idx]\n",
        "        data = np.load(file_path)\n",
        "\n",
        "        # Convert numpy array to PIL Image\n",
        "        image = Image.fromarray(data)\n",
        "\n",
        "        # Apply transformations if specified\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Get label from the subdirectory name\n",
        "        label = self.categories.index(os.path.basename(os.path.dirname(file_path)))\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "URADXNGOiFO-"
      },
      "outputs": [],
      "source": [
        "class CustomResNet(nn.Module):\n",
        "    '''\n",
        "    Our custom ResNet model.\n",
        "    '''\n",
        "    def __init__(self, num_classes=6, leak=0.2,dropout=True,dropout_rate=0.15):\n",
        "        '''\n",
        "        Initializes the CustomResNet class.\n",
        "        Input:\n",
        "            num_classes: int\n",
        "                The number of classes.\n",
        "            leak: float\n",
        "                The negative slope of the LeakyReLU activation function.\n",
        "            dropout: bool\n",
        "                Whether to use dropout or not.\n",
        "            dropout_rate: float\n",
        "                The dropout rate.\n",
        "        '''\n",
        "        super(CustomResNet, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.dropout_rate = dropout_rate\n",
        "        # Load a pre-trained ResNet\n",
        "        self.resnet = models.resnet34(pretrained=True)\n",
        "\n",
        "        # Remove the fully connected layers\n",
        "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-2])\n",
        "\n",
        "        # Custom Convolutional layers\n",
        "        # 512 channels from ResNet's last layer\n",
        "        self.conv1 = nn.Conv2d(512, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv9 = nn.Conv2d(128, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.leakyrelu = nn.LeakyReLU(negative_slope=leak)\n",
        "        self.flatten = nn.Flatten()\n",
        "        #add a dropout\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(480, num_classes)  # Assuming the output size after pooling is 7x7\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Forward pass of the network.\n",
        "        Input:\n",
        "            x: PyTorch tensor\n",
        "                The input tensor.\n",
        "        Output:\n",
        "            x: PyTorch tensor\n",
        "                The output tensor.\n",
        "        '''\n",
        "        # Pass input through ResNet layers\n",
        "        x = self.resnet(x)\n",
        "        # Pass through custom layers\n",
        "        x = self.leakyrelu(self.bn1(self.conv1(x)))\n",
        "        x = self.leakyrelu(self.bn2(self.conv2(x)))\n",
        "        if self.dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = self.leakyrelu(self.conv3(x))\n",
        "        if self.dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = self.leakyrelu(self.conv9(x))\n",
        "\n",
        "        # Flatten and pass through the fully connected layer\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ksWMom9GW8Jq"
      },
      "outputs": [],
      "source": [
        "class FaciesPredictor():\n",
        "    '''\n",
        "    The heart of the FaciesPredictor package.\n",
        "    '''\n",
        "    def __init__(self,dropout=True,dropout_rate=0.15):\n",
        "        '''\n",
        "        Initializes the FaciesPredictor class. This will help predict\n",
        "        the facies of a given dataset.\n",
        "        Input:\n",
        "            dropout: bool\n",
        "                Whether to use dropout or not.\n",
        "            dropout_rate: float\n",
        "                The dropout rate.\n",
        "        '''\n",
        "        self.dropout = dropout\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.model = CustomResNet(dropout=self.dropout,dropout_rate=self.dropout_rate)\n",
        "\n",
        "    def train(self,train_path,epochs=20,lr=0.001,weight_decay=1e-4,scheduler=False):\n",
        "        '''\n",
        "        Trains the model.\n",
        "        Input:\n",
        "            train_path: str\n",
        "                Path to the folder where the images are stored.\n",
        "        '''\n",
        "        self.train_loader, self.val_loader = self.load_and_split(train_path)\n",
        "        self.lr = lr\n",
        "        self.scheduler = scheduler\n",
        "        self.weight_decay = weight_decay\n",
        "        self.epochs = epochs\n",
        "        self.model = self.train_model()\n",
        "\n",
        "    def load_model(self,model_path):\n",
        "        '''\n",
        "        Loads a model from a file.\n",
        "        Input:\n",
        "            model_path: str\n",
        "                Path to the file containing the model.\n",
        "        Output:\n",
        "            model: PyTorch model\n",
        "                The loaded model.\n",
        "        '''\n",
        "        try:\n",
        "            self.model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "            print(f\"Model loaded from {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load model from {model_path}. Error: {e}\")\n",
        "\n",
        "    def load_and_split(self,path):\n",
        "        '''\n",
        "        Loads the images and splits them into training and validation sets.\n",
        "        Input:\n",
        "            path: str\n",
        "                Path to the folder where the images are stored.\n",
        "        Output:\n",
        "            train_loader: PyTorch DataLoader\n",
        "                The data loader that iterates over the training dataset.\n",
        "            val_loader: PyTorch DataLoader\n",
        "                The data loader that iterates over the validation dataset.\n",
        "            '''\n",
        "        self.transform = transforms.Compose([\n",
        "        transforms.Resize((70, 300)),  # Adjust the size as needed\n",
        "        transforms.ToTensor()])\n",
        "        dataset = NumpyImageDataset(path, transform=self.transform)\n",
        "        train_size = int(0.8 * len(dataset))\n",
        "        val_size = int(0.2 * len(dataset))\n",
        "        train_set, val_set = random_split(dataset, [train_size, val_size])\n",
        "        train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "        val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
        "\n",
        "        return train_loader,val_loader\n",
        "\n",
        "    def validate(self, model, criterion, data_loader):\n",
        "        '''\n",
        "        Computes the loss and accuracy of the model on the validation dataset.\n",
        "        Input:\n",
        "            model: PyTorch model\n",
        "                The model to evaluate.\n",
        "            criterion: PyTorch loss function\n",
        "                The loss function used to compute the loss.\n",
        "            data_loader: PyTorch DataLoader\n",
        "                The data loader that iterates over the validation dataset.\n",
        "        Output:\n",
        "            validation_loss: float\n",
        "                The average loss over the validation dataset.\n",
        "            validation_accuracy: float\n",
        "                The average accuracy over the validation dataset.\n",
        "        '''\n",
        "        model.eval()\n",
        "        validation_loss, validation_accuracy = 0., 0.\n",
        "        for X, y in data_loader:\n",
        "            with torch.no_grad():\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                a2 = model(X.view(-1, 3, 70, 300))\n",
        "                loss = criterion(a2, y)\n",
        "                validation_loss += loss.detach().item() * X.size(0)\n",
        "                y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
        "                validation_accuracy += accuracy_score(y.cpu().numpy(),\n",
        "                                                      y_pred.cpu().numpy()) * X.size(0)\n",
        "\n",
        "        return validation_loss / len(data_loader.dataset), validation_accuracy / len(data_loader.dataset)\n",
        "\n",
        "    def trainer(self, model, optimizer, criterion, data_loader):\n",
        "        '''\n",
        "        Trains the model for one epoch.\n",
        "        Input:\n",
        "            model: PyTorch model\n",
        "                The model to train.\n",
        "            optimizer: PyTorch optimizer\n",
        "                The optimizer used to update the model's weights.\n",
        "            criterion: PyTorch loss function\n",
        "                The loss function used to compute the loss.\n",
        "            data_loader: PyTorch DataLoader\n",
        "                The data loader that iterates over the training dataset.\n",
        "        Output:\n",
        "            train_loss: float\n",
        "                The average loss over the training dataset.\n",
        "            train_accuracy: float\n",
        "                The average accuracy over the training dataset.\n",
        "                '''\n",
        "        model.train()\n",
        "        train_loss, train_accuracy = 0, 0\n",
        "        total_samples = len(data_loader.dataset)\n",
        "\n",
        "        for i, (X, y) in enumerate(data_loader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            a2 = model(X.view(-1, 3, 70, 300))\n",
        "            loss = criterion(a2, y)\n",
        "            loss.backward()\n",
        "            train_loss += loss.detach().item() * X.size(0)\n",
        "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
        "            train_accuracy += accuracy_score(y.cpu().numpy(),\n",
        "                                             y_pred.detach().cpu().numpy()) * X.size(0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print progress\n",
        "            if (i + 1) % 10 == 0:  # Print every 10 iterations\n",
        "                print(f\"Iteration [{i + 1}/{len(data_loader)}] \"\n",
        "                      f\"Loss: {train_loss / (i + 1):.4f} \"\n",
        "                      f\"Accuracy: {train_accuracy / total_samples:.4f}\")\n",
        "        return train_loss / total_samples, train_accuracy / total_samples\n",
        "\n",
        "    def train_model(self, seed=42):\n",
        "        '''\n",
        "        Trains the model.\n",
        "        Input:\n",
        "            seed: int\n",
        "                Random seed.\n",
        "        Output:\n",
        "            model: PyTorch model\n",
        "                The trained model.\n",
        "        '''\n",
        "        set_seed(seed)\n",
        "        self.model = self.model.to(device)\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        if self.scheduler:\n",
        "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "        liveloss = PlotLosses()\n",
        "        for _ in range(self.epochs):\n",
        "            logs = {}\n",
        "            train_loss, train_accuracy = self.trainer(self.model, optimizer, criterion, self.train_loader)\n",
        "\n",
        "            logs['log loss'] = train_loss\n",
        "            logs['accuracy'] = train_accuracy\n",
        "\n",
        "            validation_loss, validation_accuracy = self.validate(self.model, criterion, self.val_loader)\n",
        "            logs['val_log loss'] = validation_loss\n",
        "            logs['val_accuracy'] = validation_accuracy\n",
        "            if self.scheduler:\n",
        "                scheduler.step()\n",
        "            liveloss.update(logs)\n",
        "            liveloss.draw()\n",
        "        return self.model\n",
        "\n",
        "    def save(self,save_path):\n",
        "        '''\n",
        "        Saves the model to a file.\n",
        "        Input:\n",
        "            save_path: str\n",
        "                Path to the folder where the model will be saved.\n",
        "        '''\n",
        "        self.save_path = save_path\n",
        "        torch.save(self.model.state_dict(), f\"{self.save_path}_model_geoprediction.pth\")\n",
        "\n",
        "    def segment(self,images, depths, rows_per_segment=140):\n",
        "        '''\n",
        "        Segments the images and depths into smaller segments.\n",
        "        Input:\n",
        "            images: numpy array\n",
        "                A numpy array containing the images.\n",
        "            depths: numpy array\n",
        "                A numpy array containing the depths.\n",
        "            rows_per_segment: int\n",
        "                The number of rows per segment.\n",
        "        Output:\n",
        "            segmented_images: list\n",
        "                A list of numpy arrays containing the segmented images.\n",
        "            segmented_depths: list\n",
        "                A list of tuples containing the start and end depths of each segment.\n",
        "        '''\n",
        "        segmented_images = []\n",
        "        segmented_depths = []\n",
        "\n",
        "        # Calculate the number of segments\n",
        "        num_segments = len(images) // rows_per_segment\n",
        "\n",
        "        for i in range(num_segments):\n",
        "            start_idx = i * rows_per_segment\n",
        "            end_idx = start_idx + rows_per_segment\n",
        "\n",
        "            # Segment the images\n",
        "            segmented_image = images[start_idx:end_idx]\n",
        "            segmented_images.append(segmented_image)\n",
        "\n",
        "            # Create the depth tuple\n",
        "            start_depth = depths[start_idx]\n",
        "            end_depth = depths[end_idx - 1]  # Use end_idx - 1 to get the last depth in the segment\n",
        "            segmented_depths.append((start_depth, end_depth))\n",
        "\n",
        "        return segmented_images, segmented_depths\n",
        "    def transform_segmented_images(self,segmented_images):\n",
        "        '''\n",
        "        Transforms the segmented images to tensors, and makes them\n",
        "        compatible with the model.\n",
        "        Input:\n",
        "            segmented_images: list\n",
        "                A list of numpy arrays containing the segmented images.\n",
        "        Output:\n",
        "            transformed_images: list\n",
        "                A list of tensors containing the transformed images.\n",
        "        '''\n",
        "        transformed_images = []\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((70, 300)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        for image in segmented_images:\n",
        "            # Convert numpy array to PIL Image\n",
        "            pil_image = Image.fromarray(image.astype(np.uint8))\n",
        "            # Apply the transformation\n",
        "            transformed_image = self.transform(pil_image)\n",
        "            transformed_images.append(transformed_image)\n",
        "\n",
        "        return transformed_images\n",
        "\n",
        "    def predict(self,npy_image_path,npy_depth_path):\n",
        "        '''\n",
        "        Predicts the facies of a given image.\n",
        "        Input:\n",
        "            npy_image_path: str\n",
        "                Path to the folder where the images are stored.\n",
        "            npy_depth_path: str\n",
        "                Path to the folder where the depths are stored.\n",
        "        Output:\n",
        "            results_df: pandas DataFrame\n",
        "                A pandas DataFrame containing the start and end\n",
        "                depths of each segment, and the predicted facies.\n",
        "        '''\n",
        "        images = np.load(npy_image_path)\n",
        "        depths = np.load(npy_depth_path)\n",
        "\n",
        "        self.segmented_images, self.segmented_depths = self.segment(images, depths)\n",
        "        transformed_images = self.transform_segmented_images(self.segmented_images)\n",
        "        class_mapping = {0: 'ih', 1: 'is', 2: 'nc', 3: 'os', 4: 's', 5: 'sh'}\n",
        "        # Assuming transformed_images is a list of tensors\n",
        "        images_batch = torch.stack(transformed_images)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        images_batch = images_batch.to(device)\n",
        "        self.model.to(device)\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():  # Disable gradient computation for inference\n",
        "            outputs = self.model(images_batch)\n",
        "            _, predicted_classes = torch.max(outputs, 1)\n",
        "        predicted_labels = [class_mapping[idx.item()] for idx in predicted_classes]\n",
        "        self.results_df = pd.DataFrame(self.segmented_depths, columns=['Start Depth', 'End Depth'])\n",
        "        self.results_df['Predicted Class'] = predicted_labels\n",
        "        return self.results_df\n",
        "\n",
        "    def export(self,export_path,name='predictions'):\n",
        "        '''\n",
        "        Exports the predictions to a CSV file.\n",
        "        Input:\n",
        "            export_path: str\n",
        "                Path to the folder where the CSV file will be saved.\n",
        "            name: str\n",
        "                Name of the CSV file.\n",
        "        '''\n",
        "        self.results_df.to_csv(f'{export_path}/{name}.csv', index=False)\n",
        "        print(f\"Predictions exported to {export_path}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "IhqP5qEiPpkB",
        "outputId": "486c2188-bccf-436a-8efb-a574ea26cf51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Start Depth    End Depth Predicted Class\n",
              "0     2105.000000  2105.023326              is\n",
              "1     2105.023494  2105.046820              is\n",
              "2     2105.046988  2105.070314              is\n",
              "3     2105.070482  2105.093808              is\n",
              "4     2105.093975  2105.117302              is\n",
              "...           ...          ...             ...\n",
              "2889  2550.860044  2550.883370              is\n",
              "2890  2550.883538  2550.906864              is\n",
              "2891  2550.907031  2550.930357              is\n",
              "2892  2550.930525  2550.953851              is\n",
              "2893  2550.954019  2550.977345              is\n",
              "\n",
              "[2894 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8a60e7a-ce21-452e-a8a5-ce302a146c24\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Start Depth</th>\n",
              "      <th>End Depth</th>\n",
              "      <th>Predicted Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2105.000000</td>\n",
              "      <td>2105.023326</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2105.023494</td>\n",
              "      <td>2105.046820</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2105.046988</td>\n",
              "      <td>2105.070314</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2105.070482</td>\n",
              "      <td>2105.093808</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2105.093975</td>\n",
              "      <td>2105.117302</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2889</th>\n",
              "      <td>2550.860044</td>\n",
              "      <td>2550.883370</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2890</th>\n",
              "      <td>2550.883538</td>\n",
              "      <td>2550.906864</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2891</th>\n",
              "      <td>2550.907031</td>\n",
              "      <td>2550.930357</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2892</th>\n",
              "      <td>2550.930525</td>\n",
              "      <td>2550.953851</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2893</th>\n",
              "      <td>2550.954019</td>\n",
              "      <td>2550.977345</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2894 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8a60e7a-ce21-452e-a8a5-ce302a146c24')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d8a60e7a-ce21-452e-a8a5-ce302a146c24 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d8a60e7a-ce21-452e-a8a5-ce302a146c24');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f637966b-8241-4fee-ab39-1afe822ace46\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f637966b-8241-4fee-ab39-1afe822ace46')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f637966b-8241-4fee-ab39-1afe822ace46 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "folder_path = '/content/gdrive/MyDrive/small_training'\n",
        "model = FaciesPredictor()\n",
        "#model.train(folder_path)\n",
        "#model.load_model('/content/gdrive/MyDrive/good_model_epoch_30.pth')\n",
        "model.predict('/content/gdrive/MyDrive/small_test/204-19-7_image.npy','/content/gdrive/MyDrive/small_test/204-19-7_depth.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j91VCvEsrrbX",
        "outputId": "7fc25bb6-dff0-441d-ec15-3c12532849f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_path=folder_path = '/content/gdrive/MyDrive/Colab Notebooks/good_model_epoch_30.pth'\n",
        "model.load_model(model_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}